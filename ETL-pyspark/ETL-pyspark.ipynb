{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e693c91a-2b2f-4b6d-9cd7-ac15edf007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+---------+---------------+\n",
      "|fecha_anio|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+----------+---------+---------------+---------+---------------+\n",
      "|      2024|3908452.0|    1.2595739E8|      336|        56108.0|\n",
      "|      2023|3509190.0|    9.5067776E7|      363|        36456.0|\n",
      "|      2022| 180628.0|      8576601.0|       58|         4959.0|\n",
      "|      2021|  20816.0|       478736.0|       13|          439.0|\n",
      "|      2020|  21502.0|       315269.0|       17|          631.0|\n",
      "|      2019|   2251.0|        24663.0|        2|          121.0|\n",
      "|      2018|    507.0|        11678.0|        1|           43.0|\n",
      "|      2017|   2716.0|        17358.0|        5|           91.0|\n",
      "|      2016|    220.0|         1812.0|        1|            0.0|\n",
      "|      2013|     48.0|            0.0|        1|            1.0|\n",
      "+----------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "|fecha_mes|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "|       01|2849720.0|    7.6373879E7|       73|        17881.0|\n",
      "|       02| 221042.0|      7444571.0|       56|         6391.0|\n",
      "|       03| 405724.0|    1.4868345E7|      112|         5133.0|\n",
      "|       04| 188848.0|    1.0538415E7|       75|         5390.0|\n",
      "|       05| 160244.0|      6064349.0|       69|         6608.0|\n",
      "|       06| 176449.0|      5691132.0|       35|         4802.0|\n",
      "|       07| 322769.0|    1.0433561E7|       64|        11538.0|\n",
      "|       08|1336308.0|    2.7575894E7|       79|        12270.0|\n",
      "|       09| 760194.0|    2.8345345E7|       99|        16600.0|\n",
      "|       10| 693599.0|     2.016929E7|       54|         5704.0|\n",
      "|       11|  91168.0|      4583878.0|       37|         2434.0|\n",
      "|       12| 440265.0|    1.8362624E7|       44|         4098.0|\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "|fecha_hora_dia_semana|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "|               Friday|2093851.0|    6.3510721E7|      103|        22600.0|\n",
      "|               Monday|1156284.0|    3.6169255E7|      127|        14661.0|\n",
      "|             Saturday| 151170.0|      8469746.0|       55|         4885.0|\n",
      "|               Sunday| 228440.0|    1.0209412E7|       78|         7129.0|\n",
      "|             Thursday|2005061.0|    6.5832587E7|      133|        18511.0|\n",
      "|              Tuesday|1400974.0|    2.6165436E7|      137|        12969.0|\n",
      "|            Wednesday| 610550.0|    2.0094126E7|      164|        18094.0|\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+\n",
      "|fecha_hora_dia_semana|2013|2016|2017|2018|2019|2020|2021|2022|2023|2024|\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+\n",
      "|               Friday|NULL|NULL|   1|NULL|NULL|   2|NULL|   9|  45|  46|\n",
      "|               Monday|NULL|NULL|NULL|NULL|   1|   4|   1|  10|  53|  58|\n",
      "|             Saturday|   1|NULL|NULL|NULL|NULL|NULL|NULL|   2|  30|  22|\n",
      "|               Sunday|NULL|NULL|   3|NULL|NULL|   4|   2|   4|  31|  34|\n",
      "|             Thursday|NULL|NULL|   1|NULL|NULL|   1|   3|  12|  62|  54|\n",
      "|              Tuesday|NULL|NULL|NULL|NULL|   1|   2|   4|   7|  67|  56|\n",
      "|            Wednesday|NULL|   1|NULL|   1|NULL|   4|   3|  14|  75|  66|\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, regexp_replace,  expr, array_join, trim, regexp_extract, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark\n",
    "import tempfile\n",
    "import collections\n",
    "import re\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EjemploPySpark\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "file = 'data_user_reels_claudianicolasa.csv'\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "'''\n",
    "El csv tiene el problema de que un campo esta ocupando una o varias filas del csv\n",
    "es_incompleta y unir_lineas_incompletas son dos metodos para solventar este problema\n",
    "es_incompleta para comprobar si el campo usa mas de una linea\n",
    "unir_lineas_incompletas para joinear esas lineas\n",
    "'''\n",
    "\n",
    "# Función para eliminar comas dentro de comillas dobles en una línea CSV\n",
    "def eliminar_comas_entre_comillas(line):\n",
    "    # Usamos una expresión regular que encuentra cualquier cosa entre comillas dobles\n",
    "    # y luego eliminamos las comas que están dentro de ese texto\n",
    "    return re.sub(r'\\\"(.*?)\\\"', lambda match: match.group(0).replace(\",\", \"\"), line)\n",
    "\n",
    "\n",
    "# Función para verificar si la línea está incompleta\n",
    "def es_incompleta(line):\n",
    "    # Verifica si la línea contiene un número impar de comillas dobles\n",
    "    return line.count('\"') % 2 != 0\n",
    "\n",
    "# Acumulamos las líneas hasta que tengamos una línea completa (con un número par de comillas)\n",
    "def unir_lineas_incompletas(rdd):\n",
    "    acumulado = []\n",
    "    resultados = []\n",
    "\n",
    "    for line in rdd.collect():\n",
    "        acumulado.append(line)\n",
    "        # Si la línea completa el conjunto de comillas\n",
    "        if not es_incompleta(\" \".join(acumulado)):\n",
    "            # Unimos las líneas acumuladas y las añadimos al resultado\n",
    "            resultados.append(\" \".join(acumulado))\n",
    "            acumulado = []\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Unir las líneas mal formadas en el RDD\n",
    "lineas_corregidas = spark.sparkContext.parallelize(unir_lineas_incompletas(lines))\n",
    "\n",
    "\n",
    "# Aplicamos la función a cada línea corregida para eliminar las comas entre comillas\n",
    "rdd_sin_comas = lineas_corregidas.map(eliminar_comas_entre_comillas)\n",
    "\n",
    "# Ahora convertimos cada línea en una lista de columnas usando split por comas\n",
    "rdd_estructurado = rdd_sin_comas.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# Creamos el schema del dataframe\n",
    "columnas = [\"id\",\"comentario\", \"like\", \"fecha\", \"tipopublicacion\", \"oldvisualizaciones\", \"visualizaciones\", \"titulo\"]     \n",
    "# Convertimos el RDD en DataFrame usando toDF y asignamos nombres de columnas\n",
    "df = spark.createDataFrame(rdd_estructurado, columnas)\n",
    "\n",
    "#df.printSchema()\n",
    "\n",
    "#PROCEDEMOS A TRANSFORMAR ALGUNAS COLUMNAS\n",
    "\n",
    "#Creamos una sola columna para las views\n",
    "df = df.withColumn(\"totalViews\", df.oldvisualizaciones + df.visualizaciones)\n",
    "\n",
    "#En cuanto al titulo lo dividimos en la parte de hastag y el resto\n",
    "df = df.withColumn(\"titulopublicacion\", split(df[\"titulo\"], \"#\").getItem(0)) \n",
    "#df = df.withColumn(\"hashtagtodos\", regexp_replace(df[\"titulo\"], df[\"titulopublicacion\"], \"-\"))\n",
    "#df.select(\"hashtagtodos\").show(truncate=False)\n",
    "#df.select(\"titulopublicacion\").show(truncate=False)\n",
    "    #.withColumn(\"hastag2\", split(df[\"titulo\"], \"#\").getItem(1))\\\n",
    "      \n",
    "#creamos nuevas columnas en funcion de la fecha de publicacion -> año, mes, dia_semana, hora\n",
    "df = df.withColumn(\"fecha_anio\", split(df[\"fecha\"], \"-\").getItem(0)) \n",
    "df = df.withColumn(\"fecha_mes\", split(df[\"fecha\"], \"-\").getItem(1)) \n",
    "df = df.withColumn(\"fecha_hora\", split(df[\"fecha\"], \" \").getItem(1)) \n",
    "\n",
    "df = df.withColumn(\"fecha_timestamp\", F.to_timestamp(F.col(\"fecha\"), \"yyyy-MM-dd HH:mm:ssXXX\"))\n",
    "df = df.withColumn(\"fecha_hora_dia_semana\", F.date_format(F.col(\"fecha_timestamp\"), \"EEEE\"))\n",
    "\n",
    "#Creamos dos nuevas columnas para el engagement por like y por comentario \n",
    "df = df.withColumn(\"engagement_like\", df.like / df.totalViews)\n",
    "df = df.withColumn(\"engagement_comentario\", df.comentario / df.totalViews)\n",
    "#df.select(\"engagement_like\", \"engagement_comentario\").show(truncate=False)\n",
    "\n",
    "#Eliminamos todas las filas que sus campos son todos nulos\n",
    "df.na.drop()\n",
    "\n",
    "df.coalesce(1) \\\n",
    "    .write.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"new_folder\")\n",
    "\n",
    "#ANALISIS\n",
    "\n",
    "df.groupBy(\"fecha_anio\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\" }).orderBy(\"fecha_anio\", ascending=False).show()\n",
    "df.groupBy(\"fecha_mes\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_mes\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_hora_dia_semana\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").pivot(\"fecha_anio\").count().orderBy(\"fecha_hora_dia_semana\").show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a205b27-c297-44ae-a0ff-fc99237efad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4bf3f-20df-4118-b340-9b2db92306a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4e3aa-0e05-4b31-9464-47fc90705056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0ddd4-92de-4d38-b67c-bb07b22aa53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
