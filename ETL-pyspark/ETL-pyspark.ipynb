{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a346ad62-ce26-4fef-8f26-b4cbd695ba62",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'oldvisualizaciones'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m\n\u001b[1;32m     77\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rdd_estructurado)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#df.printSchema()\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m#PROCEDEMOS A TRANSFORMAR ALGUNAS COLUMNAS\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#Creamos una sola columna para las views\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalViews\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moldvisualizaciones\u001b[49m \u001b[38;5;241m+\u001b[39m df\u001b[38;5;241m.\u001b[39mvisualizaciones)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#En cuanto al titulo lo dividimos en la parte de hastag y el resto\u001b[39;00m\n\u001b[1;32m     87\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitulopublicacion\u001b[39m\u001b[38;5;124m\"\u001b[39m, split(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitulo\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetItem(\u001b[38;5;241m0\u001b[39m)) \n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3091\u001b[0m \n\u001b[1;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3125\u001b[0m     )\n\u001b[1;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'oldvisualizaciones'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, regexp_replace, regexp_extract_all,  expr, array_join, trim, regexp_extract, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark\n",
    "import collections\n",
    "import re\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EjemploPySpark\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "file = 'data_user_reels_claudianicolasa.csv'\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "'''\n",
    "El csv tiene el problema de que un campo esta ocupando una o varias filas del csv\n",
    "es_incompleta y unir_lineas_incompletas son dos metodos para solventar este problema\n",
    "es_incompleta para comprobar si el campo usa mas de una linea\n",
    "unir_lineas_incompletas para joinear esas lineas\n",
    "'''\n",
    "\n",
    "# Función para eliminar comas dentro de comillas dobles en una línea CSV\n",
    "def eliminar_comas_entre_comillas(line):\n",
    "    # Usamos una expresión regular que encuentra cualquier cosa entre comillas dobles\n",
    "    # y luego eliminamos las comas que están dentro de ese texto\n",
    "    return re.sub(r'\\\"(.*?)\\\"', lambda match: match.group(0).replace(\",\", \"\"), line)\n",
    "\n",
    "\n",
    "# Función para verificar si la línea está incompleta\n",
    "def es_incompleta(line):\n",
    "    # Verifica si la línea contiene un número impar de comillas dobles\n",
    "    return line.count('\"') % 2 != 0\n",
    "\n",
    "# Acumulamos las líneas hasta que tengamos una línea completa (con un número par de comillas)\n",
    "def unir_lineas_incompletas(rdd):\n",
    "    acumulado = []\n",
    "    resultados = []\n",
    "\n",
    "    for line in rdd.collect():\n",
    "        acumulado.append(line)\n",
    "        # Si la línea completa el conjunto de comillas\n",
    "        if not es_incompleta(\" \".join(acumulado)):\n",
    "            # Unimos las líneas acumuladas y las añadimos al resultado\n",
    "            resultados.append(\" \".join(acumulado))\n",
    "            acumulado = []\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Unir las líneas mal formadas en el RDD\n",
    "lineas_corregidas = spark.sparkContext.parallelize(unir_lineas_incompletas(lines))\n",
    "\n",
    "\n",
    "# Aplicamos la función a cada línea corregida para eliminar las comas entre comillas\n",
    "rdd_sin_comas = lineas_corregidas.map(eliminar_comas_entre_comillas)\n",
    "\n",
    "# Ahora convertimos cada línea en una lista de columnas usando split por comas\n",
    "rdd_estructurado = rdd_sin_comas.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# Creamos el schema del dataframe\n",
    "\n",
    "esquema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"comentario\", StringType(), True),\n",
    "    StructField(\"like\", StringType(), True),\n",
    "    StructField(\"fecha\", StringType(), False),\n",
    "    StructField(\"tipopublicacion\", StringType(), True),\n",
    "    StructField(\"oldvisualizaciones\", StringType(), True),\n",
    "    StructField(\"visualizaciones\", StringType(), True),\n",
    "    StructField(\"titulo\", StringType(), False)\n",
    "])\n",
    "\n",
    "     \n",
    "# Convertimos el RDD en DataFrame usando toDF y asignamos nombres de columnas\n",
    "df = spark.createDataFrame(rdd_estructurado, schema=esquema)\n",
    "\n",
    "#df.printSchema()\n",
    "\n",
    "#PROCEDEMOS A TRANSFORMAR ALGUNAS COLUMNAS\n",
    "\n",
    "#Creamos una sola columna para las views\n",
    "df = df.withColumn(\"totalViews\", df.oldvisualizaciones + df.visualizaciones)\n",
    "\n",
    "#En cuanto al titulo lo dividimos en la parte de hastag y el resto\n",
    "df = df.withColumn(\"titulopublicacion\", split(df[\"titulo\"], \"#\").getItem(0)) \n",
    "df = df.withColumn(\"hashtagtodos\", regexp_replace(df[\"titulo\"], df[\"titulopublicacion\"], \"-\"))\n",
    "#df.select(\"hashtagtodos\").show(truncate=False)\n",
    "#df.select(\"titulopublicacion\").show(truncate=False)\n",
    "    #.withColumn(\"hastag2\", split(df[\"titulo\"], \"#\").getItem(1))\\\n",
    "      \n",
    "#creamos nuevas columnas en funcion de la fecha de publicacion -> año, mes, dia_semana, hora\n",
    "df = df.withColumn(\"fecha_anio\", split(df[\"fecha\"], \"-\").getItem(0)) \n",
    "df = df.withColumn(\"fecha_mes\", split(df[\"fecha\"], \"-\").getItem(1)) \n",
    "df = df.withColumn(\"fecha_hora\", split(df[\"fecha\"], \" \").getItem(1)) \n",
    "\n",
    "df = df.withColumn(\"fecha_timestamp\", F.to_timestamp(F.col(\"fecha\"), \"yyyy-MM-dd HH:mm:ssXXX\"))\n",
    "df = df.withColumn(\"fecha_hora_dia_semana\", F.date_format(F.col(\"fecha_timestamp\"), \"EEEE\"))\n",
    "\n",
    "#Creamos dos nuevas columnas para el engagement por like y por comentario \n",
    "df = df.withColumn(\"engagement_like\", df.like / df.totalViews)\n",
    "df = df.withColumn(\"engagement_comentario\", df.comentario / df.totalViews)\n",
    "#df.select(\"engagement_like\", \"engagement_comentario\").show(truncate=False)\n",
    "\n",
    "#Eliminamos todas las filas que sus campos son todos nulos\n",
    "df.na.drop()\n",
    "\n",
    "\n",
    "#ANALISIS\n",
    "\n",
    "df.groupBy(\"fecha_anio\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\" }).orderBy(\"fecha_anio\", ascending=False).show()\n",
    "df.groupBy(\"fecha_mes\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_mes\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_hora_dia_semana\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").pivot(\"fecha_anio\").count().orderBy(\"fecha_hora_dia_semana\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a205b27-c297-44ae-a0ff-fc99237efad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4bf3f-20df-4118-b340-9b2db92306a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4e3aa-0e05-4b31-9464-47fc90705056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0ddd4-92de-4d38-b67c-bb07b22aa53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
