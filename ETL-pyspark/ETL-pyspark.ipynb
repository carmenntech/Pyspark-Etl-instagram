{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a346ad62-ce26-4fef-8f26-b4cbd695ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+---------+---------------+\n",
      "|fecha_anio|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+----------+---------+---------------+---------+---------------+\n",
      "|     fecha|     NULL|           NULL|        1|           NULL|\n",
      "|      2024|3908452.0|    1.2595739E8|      336|        56108.0|\n",
      "|      2023|3509190.0|    9.5067776E7|      363|        36456.0|\n",
      "|      2022| 180628.0|      8576601.0|       58|         4959.0|\n",
      "|      2021|  20816.0|       478736.0|       13|          439.0|\n",
      "|      2020|  21502.0|       315269.0|       17|          631.0|\n",
      "|      2019|   2251.0|        24663.0|        2|          121.0|\n",
      "|      2018|    507.0|        11678.0|        1|           43.0|\n",
      "|      2017|   2716.0|        17358.0|        5|           91.0|\n",
      "|      2016|    220.0|         1812.0|        1|            0.0|\n",
      "|      2013|     48.0|            0.0|        1|            1.0|\n",
      "+----------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "|fecha_mes|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "|     NULL|     NULL|           NULL|        1|           NULL|\n",
      "|       01|2849720.0|    7.6373879E7|       73|        17881.0|\n",
      "|       02| 221042.0|      7444571.0|       56|         6391.0|\n",
      "|       03| 405724.0|    1.4868345E7|      112|         5133.0|\n",
      "|       04| 188848.0|    1.0538415E7|       75|         5390.0|\n",
      "|       05| 160244.0|      6064349.0|       69|         6608.0|\n",
      "|       06| 176449.0|      5691132.0|       35|         4802.0|\n",
      "|       07| 322769.0|    1.0433561E7|       64|        11538.0|\n",
      "|       08|1336308.0|    2.7575894E7|       79|        12270.0|\n",
      "|       09| 760194.0|    2.8345345E7|       99|        16600.0|\n",
      "|       10| 693599.0|     2.016929E7|       54|         5704.0|\n",
      "|       11|  91168.0|      4583878.0|       37|         2434.0|\n",
      "|       12| 440265.0|    1.8362624E7|       44|         4098.0|\n",
      "+---------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "|fecha_hora_dia_semana|sum(like)|sum(totalViews)|count(id)|sum(comentario)|\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "|                 NULL|     NULL|           NULL|        1|           NULL|\n",
      "|               Friday|2093851.0|    6.3510721E7|      103|        22600.0|\n",
      "|               Monday|1156284.0|    3.6169255E7|      127|        14661.0|\n",
      "|             Saturday| 151170.0|      8469746.0|       55|         4885.0|\n",
      "|               Sunday| 228440.0|    1.0209412E7|       78|         7129.0|\n",
      "|             Thursday|2005061.0|    6.5832587E7|      133|        18511.0|\n",
      "|              Tuesday|1400974.0|    2.6165436E7|      137|        12969.0|\n",
      "|            Wednesday| 610550.0|    2.0094126E7|      164|        18094.0|\n",
      "+---------------------+---------+---------------+---------+---------------+\n",
      "\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+-----+\n",
      "|fecha_hora_dia_semana|2013|2016|2017|2018|2019|2020|2021|2022|2023|2024|fecha|\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+-----+\n",
      "|                 NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|    1|\n",
      "|               Friday|NULL|NULL|   1|NULL|NULL|   2|NULL|   9|  45|  46| NULL|\n",
      "|               Monday|NULL|NULL|NULL|NULL|   1|   4|   1|  10|  53|  58| NULL|\n",
      "|             Saturday|   1|NULL|NULL|NULL|NULL|NULL|NULL|   2|  30|  22| NULL|\n",
      "|               Sunday|NULL|NULL|   3|NULL|NULL|   4|   2|   4|  31|  34| NULL|\n",
      "|             Thursday|NULL|NULL|   1|NULL|NULL|   1|   3|  12|  62|  54| NULL|\n",
      "|              Tuesday|NULL|NULL|NULL|NULL|   1|   2|   4|   7|  67|  56| NULL|\n",
      "|            Wednesday|NULL|   1|NULL|   1|NULL|   4|   3|  14|  75|  66| NULL|\n",
      "+---------------------+----+----+----+----+----+----+----+----+----+----+-----+\n",
      "\n",
      "_______________________________________________________________________________________\n",
      "Las publicaciones mas controversicas\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `t` cannot be resolved. Did you mean one of the following? [`id`, `like`, `fecha`, `titulo`, `comentario`].;\n'Project ['t]\n+- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, fecha_hora_dia_semana#8000, engagement_like#8017, (cast(comentario#7894 as double) / totalViews#7909) AS engagement_comentario#8035]\n   +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, fecha_hora_dia_semana#8000, (cast(like#7895 as double) / totalViews#7909) AS engagement_like#8017]\n      +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, date_format(fecha_timestamp#7984, EEEE, Some(Etc/UTC)) AS fecha_hora_dia_semana#8000]\n         +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, to_timestamp(fecha#7896, Some(yyyy-MM-dd HH:mm:ssXXX), TimestampType, Some(Etc/UTC), false) AS fecha_timestamp#7984]\n            +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, split(fecha#7896,  , -1)[1] AS fecha_hora#7969]\n               +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, split(fecha#7896, -, -1)[1] AS fecha_mes#7955]\n                  +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, split(fecha#7896, -, -1)[0] AS fecha_anio#7942]\n                     +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, regexp_replace(titulo#7900, titulopublicacion#7919, ---, 1) AS hashtagtodos#7930]\n                        +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, split(titulo#7900, #, -1)[0] AS titulopublicacion#7919]\n                           +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, (cast(oldvisualizaciones#7898 as double) + cast(visualizaciones#7899 as double)) AS totalViews#7909]\n                              +- LogicalRDD [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_______________________________________________________________________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLas publicaciones mas controversicas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomentario\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `t` cannot be resolved. Did you mean one of the following? [`id`, `like`, `fecha`, `titulo`, `comentario`].;\n'Project ['t]\n+- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, fecha_hora_dia_semana#8000, engagement_like#8017, (cast(comentario#7894 as double) / totalViews#7909) AS engagement_comentario#8035]\n   +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, fecha_hora_dia_semana#8000, (cast(like#7895 as double) / totalViews#7909) AS engagement_like#8017]\n      +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, fecha_timestamp#7984, date_format(fecha_timestamp#7984, EEEE, Some(Etc/UTC)) AS fecha_hora_dia_semana#8000]\n         +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, fecha_hora#7969, to_timestamp(fecha#7896, Some(yyyy-MM-dd HH:mm:ssXXX), TimestampType, Some(Etc/UTC), false) AS fecha_timestamp#7984]\n            +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, fecha_mes#7955, split(fecha#7896,  , -1)[1] AS fecha_hora#7969]\n               +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, fecha_anio#7942, split(fecha#7896, -, -1)[1] AS fecha_mes#7955]\n                  +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, hashtagtodos#7930, split(fecha#7896, -, -1)[0] AS fecha_anio#7942]\n                     +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, titulopublicacion#7919, regexp_replace(titulo#7900, titulopublicacion#7919, ---, 1) AS hashtagtodos#7930]\n                        +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, totalViews#7909, split(titulo#7900, #, -1)[0] AS titulopublicacion#7919]\n                           +- Project [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900, (cast(oldvisualizaciones#7898 as double) + cast(visualizaciones#7899 as double)) AS totalViews#7909]\n                              +- LogicalRDD [id#7893, comentario#7894, like#7895, fecha#7896, tipopublicacion#7897, oldvisualizaciones#7898, visualizaciones#7899, titulo#7900], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, regexp_replace, regexp_extract_all,  expr, array_join, trim, regexp_extract, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark\n",
    "import collections\n",
    "import re\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EjemploPySpark\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "file = 'data_user_reels_claudianicolasa.csv'\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "'''\n",
    "El csv tiene el problema de que un campo esta ocupando una o varias filas del csv\n",
    "es_incompleta y unir_lineas_incompletas son dos metodos para solventar este problema\n",
    "es_incompleta para comprobar si el campo usa mas de una linea\n",
    "unir_lineas_incompletas para joinear esas lineas\n",
    "'''\n",
    "\n",
    "# Función para eliminar comas dentro de comillas dobles en una línea CSV\n",
    "def eliminar_comas_entre_comillas(line):\n",
    "    # Usamos una expresión regular que encuentra cualquier cosa entre comillas dobles\n",
    "    # y luego eliminamos las comas que están dentro de ese texto\n",
    "    return re.sub(r'\\\"(.*?)\\\"', lambda match: match.group(0).replace(\",\", \"\"), line)\n",
    "\n",
    "\n",
    "# Función para verificar si la línea está incompleta\n",
    "def es_incompleta(line):\n",
    "    # Verifica si la línea contiene un número impar de comillas dobles\n",
    "    return line.count('\"') % 2 != 0\n",
    "\n",
    "# Acumulamos las líneas hasta que tengamos una línea completa (con un número par de comillas)\n",
    "def unir_lineas_incompletas(rdd):\n",
    "    acumulado = []\n",
    "    resultados = []\n",
    "\n",
    "    for line in rdd.collect():\n",
    "        acumulado.append(line)\n",
    "        # Si la línea completa el conjunto de comillas\n",
    "        if not es_incompleta(\" \".join(acumulado)):\n",
    "            # Unimos las líneas acumuladas y las añadimos al resultado\n",
    "            resultados.append(\" \".join(acumulado))\n",
    "            acumulado = []\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Unir las líneas mal formadas en el RDD\n",
    "lineas_corregidas = spark.sparkContext.parallelize(unir_lineas_incompletas(lines))\n",
    "\n",
    "\n",
    "# Aplicamos la función a cada línea corregida para eliminar las comas entre comillas\n",
    "rdd_sin_comas = lineas_corregidas.map(eliminar_comas_entre_comillas)\n",
    "\n",
    "# Ahora convertimos cada línea en una lista de columnas usando split por comas\n",
    "rdd_estructurado = rdd_sin_comas.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# Creamos el schema del dataframe\n",
    "\n",
    "esquema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"comentario\", StringType(), True),\n",
    "    StructField(\"like\", StringType(), True),\n",
    "    StructField(\"fecha\", StringType(), False),\n",
    "    StructField(\"tipopublicacion\", StringType(), True),\n",
    "    StructField(\"oldvisualizaciones\", StringType(), True),\n",
    "    StructField(\"visualizaciones\", StringType(), True),\n",
    "    StructField(\"titulo\", StringType(), False)\n",
    "])\n",
    "\n",
    "     \n",
    "# Convertimos el RDD en DataFrame usando toDF y asignamos nombres de columnas\n",
    "df = spark.createDataFrame(rdd_estructurado, schema=esquema)\n",
    "\n",
    "#df.printSchema()\n",
    "\n",
    "#PROCEDEMOS A TRANSFORMAR ALGUNAS COLUMNAS\n",
    "\n",
    "#Creamos una sola columna para las views\n",
    "df = df.withColumn(\"totalViews\", df.oldvisualizaciones + df.visualizaciones)\n",
    "\n",
    "#En cuanto al titulo lo dividimos en la parte de hastag y el resto\n",
    "df = df.withColumn(\"titulopublicacion\", split(df[\"titulo\"], \"#\").getItem(0)) \n",
    "df = df.withColumn(\"hashtagtodos\", regexp_replace(df[\"titulo\"], df[\"titulopublicacion\"], \"---\"))\n",
    "#df.select(\"hashtagtodos\").show(truncate=False)\n",
    "#df.select(\"titulopublicacion\").show(truncate=False)\n",
    "    #.withColumn(\"hastag2\", split(df[\"titulo\"], \"#\").getItem(1))\\\n",
    "      \n",
    "#creamos nuevas columnas en funcion de la fecha de publicacion -> año, mes, dia_semana, hora\n",
    "df = df.withColumn(\"fecha_anio\", split(df[\"fecha\"], \"-\").getItem(0)) \n",
    "df = df.withColumn(\"fecha_mes\", split(df[\"fecha\"], \"-\").getItem(1)) \n",
    "df = df.withColumn(\"fecha_hora\", split(df[\"fecha\"], \" \").getItem(1)) \n",
    "\n",
    "df = df.withColumn(\"fecha_timestamp\", F.to_timestamp(F.col(\"fecha\"), \"yyyy-MM-dd HH:mm:ssXXX\"))\n",
    "df = df.withColumn(\"fecha_hora_dia_semana\", F.date_format(F.col(\"fecha_timestamp\"), \"EEEE\"))\n",
    "\n",
    "#Creamos dos nuevas columnas para el engagement por like y por comentario \n",
    "df = df.withColumn(\"engagement_like\", df.like / df.totalViews)\n",
    "df = df.withColumn(\"engagement_comentario\", df.comentario / df.totalViews)\n",
    "#df.select(\"engagement_like\", \"engagement_comentario\").show(truncate=False)\n",
    "\n",
    "#Eliminamos todas las filas que sus campos son todos nulos\n",
    "df.na.drop()\n",
    "\n",
    "\n",
    "#ANALISIS\n",
    "\n",
    "df.groupBy(\"fecha_anio\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\" }).orderBy(\"fecha_anio\", ascending=False).show()\n",
    "df.groupBy(\"fecha_mes\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_mes\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").agg({\"totalViews\":\"sum\",\"like\":\"sum\", \"comentario\":\"sum\", \"id\": \"count\"  }).orderBy(\"fecha_hora_dia_semana\").show()\n",
    "df.groupBy(\"fecha_hora_dia_semana\").pivot(\"fecha_anio\").count().orderBy(\"fecha_hora_dia_semana\").show()\n",
    "\n",
    "print(\"_______________________________________________________________________________________\")\n",
    "\n",
    "#df.select(max(\"comentario\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a205b27-c297-44ae-a0ff-fc99237efad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4bf3f-20df-4118-b340-9b2db92306a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4e3aa-0e05-4b31-9464-47fc90705056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0ddd4-92de-4d38-b67c-bb07b22aa53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
