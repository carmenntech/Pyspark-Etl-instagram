{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a346ad62-ce26-4fef-8f26-b4cbd695ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- comentario: integer (nullable = true)\n",
      " |-- like: integer (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- tipopublicacion: integer (nullable = true)\n",
      " |-- oldvisualizaciones: integer (nullable = true)\n",
      " |-- visualizaciones: integer (nullable = true)\n",
      " |-- titulo: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import collections\n",
    "import re\n",
    "from pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EjemploPySpark\").getOrCreate()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "file = 'data_user_reels_claudianicolasa.csv'\n",
    "\n",
    "lines = spark.sparkContext.textFile(file)\n",
    "\n",
    "'''\n",
    "El csv tiene el problema de que un campo esta ocupando una o varias filas del csv\n",
    "es_incompleta y unir_lineas_incompletas son dos metodos para solventar este problema\n",
    "es_incompleta para comprobar si el campo usa mas de una linea\n",
    "unir_lineas_incompletas para joinear esas lineas\n",
    "'''\n",
    "\n",
    "# Función para eliminar comas dentro de comillas dobles en una línea CSV\n",
    "def eliminar_comas_entre_comillas(line):\n",
    "    # Usamos una expresión regular que encuentra cualquier cosa entre comillas dobles\n",
    "    # y luego eliminamos las comas que están dentro de ese texto\n",
    "    return re.sub(r'\\\"(.*?)\\\"', lambda match: match.group(0).replace(\",\", \"\"), line)\n",
    "\n",
    "\n",
    "# Función para verificar si la línea está incompleta\n",
    "def es_incompleta(line):\n",
    "    # Verifica si la línea contiene un número impar de comillas dobles\n",
    "    return line.count('\"') % 2 != 0\n",
    "\n",
    "# Acumulamos las líneas hasta que tengamos una línea completa (con un número par de comillas)\n",
    "def unir_lineas_incompletas(rdd):\n",
    "    acumulado = []\n",
    "    resultados = []\n",
    "\n",
    "    for line in rdd.collect():\n",
    "        acumulado.append(line)\n",
    "        # Si la línea completa el conjunto de comillas\n",
    "        if not es_incompleta(\" \".join(acumulado)):\n",
    "            # Unimos las líneas acumuladas y las añadimos al resultado\n",
    "            resultados.append(\" \".join(acumulado))\n",
    "            acumulado = []\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Unir las líneas mal formadas en el RDD\n",
    "lineas_corregidas = spark.sparkContext.parallelize(unir_lineas_incompletas(lines))\n",
    "\n",
    "\n",
    "# Aplicamos la función a cada línea corregida para eliminar las comas entre comillas\n",
    "rdd_sin_comas = lineas_corregidas.map(eliminar_comas_entre_comillas)\n",
    "\n",
    "# Ahora convertimos cada línea en una lista de columnas usando split por comas\n",
    "rdd_estructurado = rdd_sin_comas.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# Creamos el schema del dataframe\n",
    "\n",
    "esquema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"comentario\", IntegerType(), True),\n",
    "    StructField(\"like\", IntegerType(), True),\n",
    "    StructField(\"fecha\", DateType(), True),\n",
    "    StructField(\"tipopublicacion\", IntegerType(), True),\n",
    "    StructField(\"oldvisualizaciones\", IntegerType(), True),\n",
    "    StructField(\"visualizaciones\", IntegerType(), True),\n",
    "    StructField(\"titulo\", StringType(), False)\n",
    "])\n",
    "\n",
    "     \n",
    "# Convertimos el RDD en DataFrame usando toDF y asignamos nombres de columnas\n",
    "df = spark.createDataFrame(rdd_estructurado, schema=esquema)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn(\"totalViews\", df.oldvisualizaciones + df.visualizaciones)\n",
    "\n",
    "#df.show(10)\n",
    "\n",
    "#df = df.withColumn(\"titulopublicacion\", split(df[\"titulo\"], \"#\").getItem(0)) \\\n",
    "#       .withColumn(\"hastag\", split(df[\"titulo\"], \"#\").getItem(1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a205b27-c297-44ae-a0ff-fc99237efad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4bf3f-20df-4118-b340-9b2db92306a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e4e3aa-0e05-4b31-9464-47fc90705056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0ddd4-92de-4d38-b67c-bb07b22aa53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
